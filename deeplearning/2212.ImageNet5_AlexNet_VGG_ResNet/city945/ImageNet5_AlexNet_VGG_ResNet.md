| Name     | Config                                             | Result                                       | Conclusion                                                                                                                                            |
| -------- | -------------------------------------------------- | -------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------- |
| AlexNet  |                                                    |                                              |                                                                                                                                                       |
| baseline | batch_size=32 lr=0.0002 max_epoch=20               | train acc 0.810, test acc 0.871 converge 16  |                                                                                                                                                       |
|          | batch_size=256 lr=0.002                            | 不收敛                                       |                                                                                                                                                       |
|          | drop transforms.Normalize, batch_size=256 lr=0.002 | 不收敛                                       |                                                                                                                                                       |
|          | batch_size=256                                     | train acc 0.800, test acc 0.862 converge 16  |                                                                                                                                                       |
|          | batch_size=256 lr=0.00002                          | train acc 0.684, test acc 0.757 converge 20+ |                                                                                                                                                       |
|          | init_weights=False lr=0.002                        | 不收敛                                       | lr 过大会导致不收敛，此时用大 batch_size 和大 lr 也没用，用小学习率虽然收敛慢能保证先看到loss下降；与 transforms.Normalize 无关，与 init_weights 无关 |
|          | init_weights=False                                 | train acc 0.813, test acc 0.875 converge 20  |                                                                                                                                                       |
|          | drop dropout                                       | train acc 0.824, test acc 0.872 converge 17  | 都没什么用，也有可能是此模型和数据集跑不出来效果                                                                                                      |

---

| Name     | Config                                             | Result                                       | Conclusion                                                                                                                                            |
| -------- | -------------------------------------------------- | -------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------- |
| VGG16    |                                                    |                                              |                                                                                                                                                       |
| baseline | batch_size=16 lr=0.0002 max_epoch=20               | train acc 0.779, test acc 0.855 converge 20+                                             |                                                                                                                                                       |
| finetune | last layer               | train acc 0.957, test acc 0.989 converge 4                                             |                                                                                                                                                       |
| finetune | whole net               | train acc 0.906, test acc 0.972 converge 6                                             |      只调整最后一层效果更好，大数据集预训练微调非常有效                                                                                                                                                 |
| VGG16    |          Flower5                                          |                                              |                                                                                                                                                       |
| baseline | batch_size=16 lr=0.0002 max_epoch=20               | train acc 0.652, test acc 0.695 converge 19                                             |                                                                                                                                                       |
| finetune | last layer               | train acc 0.858, test acc 0.843 converge 7                                             |                                                                                                                                                       |
| finetune | whole net               | train acc 0.874, test acc 0.923 converge 11                                             |                                                                                                                                                       |

